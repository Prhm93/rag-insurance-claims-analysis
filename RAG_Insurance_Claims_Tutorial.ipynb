{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline for Motor Insurance Claim Analysis\n",
    "\n",
    "A step-by-step tutorial building a Retrieval-Augmented Generation system that reads insurance PDFs and answers questions about them.\n",
    "\n",
    "**Author:** Parham Imanzadeh Charandabi | University of Salford\n",
    "\n",
    "## What We're Building\n",
    "\n",
    "A system that:\n",
    "1. **Loads** insurance PDF documents\n",
    "2. **Splits** them into manageable chunks\n",
    "3. **Embeds** each chunk into a vector (list of numbers capturing meaning)\n",
    "4. **Stores** vectors in a FAISS index for fast similarity search\n",
    "5. **Retrieves** the most relevant chunks when a user asks a question\n",
    "6. **Generates** a natural language answer using Google Gemini AI\n",
    "\n",
    "This is RAG: **R**etrieval-**A**ugmented **G**eneration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Install Libraries\n",
    "\n",
    "Each library has a specific job:\n",
    "- **langchain** — Orchestration framework that connects all RAG components\n",
    "- **langchain-google-genai** — Connects LangChain to Google's Gemini AI\n",
    "- **langchain-community** — Extra tools like PDF loaders\n",
    "- **langchain-text-splitters** — Splits documents into chunks\n",
    "- **faiss-cpu** — Facebook's fast vector similarity search (our 'smart filing cabinet')\n",
    "- **pypdf** — Reads PDF files\n",
    "- **google-generativeai** — Google's AI library for Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-google-genai langchain-community langchain-text-splitters faiss-cpu pypdf google-generativeai -q\n",
    "\n",
    "print(\"All libraries installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Upload PDFs\n",
    "\n",
    "Upload the 5 sample insurance documents from the `sample_docs/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Create a folder to store our documents\n",
    "os.makedirs(\"insurance_docs\", exist_ok=True)\n",
    "\n",
    "print(\"Click the button below to upload your 5 PDF files:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Move uploaded files to our folder\n",
    "for filename in uploaded:\n",
    "    with open(f\"insurance_docs/{filename}\", \"wb\") as f:\n",
    "        f.write(uploaded[filename])\n",
    "    print(f\"  Saved: {filename}\")\n",
    "\n",
    "print(f\"\\nTotal files uploaded: {len(uploaded)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Set Up API Key\n",
    "\n",
    "Your 'password' to use Google Gemini AI.\n",
    "Get a free key from https://aistudio.google.com/apikey\n",
    "\n",
    "**Security tip:** Use Colab's Secrets feature (key icon in left sidebar) instead of pasting directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# OPTION 1 (Recommended - Secure):\n",
    "# Click the KEY icon in Colab's left sidebar\n",
    "# Add a secret called GOOGLE_API_KEY with your key\n",
    "# Then uncomment the next two lines:\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# OPTION 2 (Quick testing only - delete before sharing):\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"PASTE_YOUR_API_KEY_HERE\"\n",
    "\n",
    "# Verify\n",
    "if os.environ.get(\"GOOGLE_API_KEY\") and os.environ[\"GOOGLE_API_KEY\"] != \"PASTE_YOUR_API_KEY_HERE\":\n",
    "    print(\"API Key is set! Ready to go.\")\n",
    "else:\n",
    "    print(\"WARNING: Please set your API key above before continuing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Load PDFs (RAG Step 1: LOAD)\n",
    "\n",
    "PyPDFLoader reads each PDF and extracts the text.\n",
    "It also tracks which file and page each piece came from (metadata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "all_documents = []\n",
    "\n",
    "for filename in sorted(os.listdir(\"insurance_docs\")):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        filepath = f\"insurance_docs/{filename}\"\n",
    "        loader = PyPDFLoader(filepath)\n",
    "        pages = loader.load()\n",
    "        all_documents.extend(pages)\n",
    "        print(f\"Loaded: {filename} ({len(pages)} pages)\")\n",
    "\n",
    "print(f\"\\nTotal pages loaded: {len(all_documents)}\")\n",
    "\n",
    "# Peek at the first document\n",
    "print(f\"\\n--- Example ---\")\n",
    "print(f\"Source: {all_documents[0].metadata['source']}\")\n",
    "print(f\"Preview: {all_documents[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Split into Chunks (RAG Step 2: CHUNK)\n",
    "\n",
    "We split documents into smaller pieces (~500 characters each) so we can\n",
    "retrieve only the relevant parts instead of feeding entire documents to the AI.\n",
    "\n",
    "- **chunk_size=500**: Each chunk is about 100 words\n",
    "- **chunk_overlap=50**: Prevents cutting sentences in half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(all_documents)\n",
    "\n",
    "print(f\"Split {len(all_documents)} pages into {len(chunks)} chunks\")\n",
    "print(f\"\\n--- Example chunk ---\")\n",
    "print(f\"From: {chunks[0].metadata['source']}\")\n",
    "print(f\"Text: {chunks[0].page_content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Create Embeddings (RAG Step 3: EMBED)\n",
    "\n",
    "This is the magic step. An **embedding** converts text into a vector\n",
    "(list of numbers) that captures its meaning.\n",
    "\n",
    "- \"vehicle hire cost\" → [0.23, -0.41, 0.87, ...]\n",
    "- \"car rental price\" → [0.21, -0.39, 0.85, ...] (similar meaning = similar numbers!)\n",
    "- \"the weather today\" → [0.91, 0.12, -0.55, ...] (different meaning = different numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/gemini-embedding-001\"\n",
    ")\n",
    "\n",
    "# Test with a simple example\n",
    "test_embedding = embedding_model.embed_query(\"vehicle hire rate\")\n",
    "print(f\"The phrase 'vehicle hire rate' becomes a vector of {len(test_embedding)} numbers\")\n",
    "print(f\"First 10 numbers: {[round(x, 4) for x in test_embedding[:10]]}\")\n",
    "print(f\"\\nThink of it as coordinates in a {len(test_embedding)}-dimensional space!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Store in FAISS (RAG Step 4: STORE)\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) stores all our vectors and lets us\n",
    "search by meaning, not just keywords.\n",
    "\n",
    "This embeds ALL chunks and indexes them. May take 30-60 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print(\"Creating vector store (may take 30-60 seconds)...\")\n",
    "\n",
    "vector_store = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "print(f\"Vector store created with {len(chunks)} entries!\")\n",
    "print(\"Documents are now searchable by meaning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Test Retrieval (RAG Step 5: RETRIEVE)\n",
    "\n",
    "Before connecting the AI, let's verify FAISS can find relevant chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TESTING RETRIEVAL (search without AI)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "query = \"What was the daily hire rate for the Ford Focus?\"\n",
    "results = vector_store.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"\\nQuery: '{query}'\")\n",
    "print(f\"Found {len(results)} relevant chunks:\\n\")\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    source = result.metadata['source'].split('/')[-1]\n",
    "    print(f\"  Result {i+1} (from {source}):\")\n",
    "    print(f\"  {result.page_content[:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Build RAG Pipeline (RAG Step 6: GENERATE)\n",
    "\n",
    "Now we connect everything: question → FAISS retrieval → prompt → Gemini → answer.\n",
    "\n",
    "We build this manually (instead of using LangChain's RetrievalQA chain) so every\n",
    "step is transparent and understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Connect to Gemini AI\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "def ask(question):\n",
    "    \"\"\"Ask a question about the insurance documents.\"\"\"\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # STEP 1: Retrieve — find the 4 most relevant chunks\n",
    "    relevant_chunks = vector_store.similarity_search(question, k=4)\n",
    "\n",
    "    # STEP 2: Build context — combine chunks into one text block\n",
    "    context = \"\"\n",
    "    sources = set()\n",
    "    for chunk in relevant_chunks:\n",
    "        context += chunk.page_content + \"\\n\\n\"\n",
    "        sources.add(chunk.metadata['source'].split('/')[-1])\n",
    "\n",
    "    # STEP 3: Create prompt — instructions + context + question\n",
    "    prompt = f\"\"\"You are an expert motor insurance claims analyst working for Whichrate Consulting Ltd.\n",
    "Use ONLY the documents below to answer. If the answer is not in the documents, say\n",
    "\\\"I cannot find this information in the available documents.\\\"\n",
    "Cite which document your answer comes from. Be precise with numbers, dates, and amounts.\n",
    "\n",
    "DOCUMENTS:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "    # STEP 4: Generate — send to Gemini and get the answer\n",
    "    response = model.generate_content(prompt)\n",
    "    answer = response.text\n",
    "\n",
    "    print(f\"\\nA: {answer.strip()}\")\n",
    "    print(f\"\\nSources: {', '.join(sources)}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "print(\"RAG pipeline ready! The ask() function is now available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Test with Example Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(\"What was the total claim value for Mrs Sarah Thompson's case?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(\"How does the CHO rate compare to the BHR average for Insurance Group 18?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(\"What fraud indicators were identified in the Ahmed Hassan claim?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(\"What is the maximum daily hire rate for a BMW 3 Series according to the policy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(\"What did the witness say about the BMW driver's behaviour?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Interactive Mode\n",
    "\n",
    "Type your own questions! Type 'quit' to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"INTERACTIVE MODE - Ask anything about the insurance claims\")\n",
    "print(\"Type 'quit' to stop\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "while True:\n",
    "    question = input(\"\\nYour question: \")\n",
    "    if question.lower() in ['quit', 'exit', 'q']:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    if question.strip():\n",
    "        ask(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Save Vector Store\n",
    "\n",
    "Save the FAISS index so you don't need to re-process PDFs next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "vector_store.save_local(\"insurance_faiss_index\")\n",
    "print(\"Vector store saved!\")\n",
    "print(\"To reload later (skip cells 4-7):\")\n",
    "print('  vector_store = FAISS.load_local(\"insurance_faiss_index\", embedding_model, allow_dangerous_deserialization=True)')"
   ]
  }
 ]
}
